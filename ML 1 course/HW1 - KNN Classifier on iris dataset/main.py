import matplotlib.pyplot as plt
from GenerateSamples import generate_samples
from knnexec import knnexec


def main():
    """
    Generate train and test sets as required, and using a 1-NN classifier, checks the error rate
    for those sets.
    """
    def question1to4():
        train_set = generate_samples(700, 1)
        test_set = generate_samples(300, 1)
        train_error = knnexec(1, train_set, test_set, 1)
        test_error = knnexec(1, train_set, test_set, 0)
        print(train_error)
        print(test_error)

    """
    The same process from question 4, but for K={1,...,20}
    Finally plots a graph with two lines showing the connection between K and the sets error rates.
    """
    def question5():
        train_set = generate_samples(700, 0)
        test_set = generate_samples(300, 0)
        error_on_train_set = []
        error_on_test_set = []
        ax = [i for i in range(1, 21)]
        for i in range(1, 21):
            error_on_train_set.append(knnexec(i, train_set, test_set, 1))
            error_on_test_set.append(knnexec(i, train_set, test_set, 0))
        plt.plot(ax, error_on_train_set, label="Train")
        plt.plot(ax, error_on_test_set, label="Test")
        plt.xlabel("K")
        plt.ylabel("Error rate")
        plt.title("Error rate to K ratio")
        plt.legend()
        plt.show()

    """
    The same process from question 4, only for different sizes of train samples.
    K=10 and m_test = 100 are constants.
    Plots a graph showing the connection between the train set size and both sets error rates.
    """
    def question6():
        m_train = [i for i in range(10, 45, 5)]
        m_test = 100
        k = 10
        error_rate_train = []
        error_rate_test = []
        for m in m_train:
            train_set = generate_samples(m, 0)
            test_set = generate_samples(m_test, 0)
            error_rate_train.append(knnexec(k, train_set, test_set, 1))
            error_rate_test.append(knnexec(k, train_set, test_set, 0))
        plt.plot(m_train, error_rate_train, label="Train")
        plt.plot(m_train, error_rate_test, label="Test")
        plt.xlabel("M-Train")
        plt.ylabel("Error rate")
        plt.title("Error rate to number of training samples")
        plt.legend()
        plt.show()

    """
    Preforms question 6 several times, to see the difference between the graphs created.
    """
    def question7():
        for i in range(6):
            question6()
    # question1to4()
    # Answer for question 4: the error rate when we predicted the train set is 0, which is to be
    # expected, since 1-nn classifier will predict a sample's label by the same point it is testing, since
    # that point is the closest single neighbor to it, and 1-nn is looking for a single closest neighbor.
    # on the other hand, the test set error rate is 0.2666, it's not too good because 1-nn classier
    # have a problem of overfitting in context to the train set.

    # question5()
    # Answer for question 5:
    # We can see that the test error does decrease as K grows larger.
    # This will not always be the case, a counter example will be:
    # we will take 3 dots as the train set, such that x = (x,y) on the axis, and the lables are black or white.
    # train set: ((1,0,black),(0,1,black),(0,0,white)), test set:(0,0,white)
    # for K=1 , the classifier will classify the test point (0,0) as white, but for K=3, it will classify
    # it as black (since the majority of points are black)
    # This usually happens when the K is too big and there are not enough samples in the train set
    # compared to the big K chosen.

    # question6()
    # Answer for question 6:
    # When K=10, we will have the problem we saw in question 5, when we don't have
    # enough samples for a K that is too big. We predict that as the number of samples increase,
    # the error rate will decrease.
    #
    # When we look at the graph we see that this is not always the case.
    # In general the error rate of both test and train is decreasing, but not for every
    # iteration (when the number of train samples increase).

    question7()
    # Answer for question 7:
    # The lines change greatly for every trial, but in general, the error rate keep
    # decreasing as the number of train samples increase.
    # The drastic changes between the lines in each trial can be explained
    # by the randomness of the test and train points generated.
    # The randomness is highly noticeable since we generate only 10-40 train samples. should we generate more
    # they will look more alike, since they are generated by the same distributions with constant parameters.

    # Answer for question 8:
    # A K-NN classifier that for every test sample point, the averages the Euclidian distance
    # from it's chosen neighbors that are assigned to the same label.
    # We will then assign the label with the smallest average as the predicted label to the test sample.


if __name__ == '__main__':
    main()
